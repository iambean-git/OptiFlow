{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preamble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/j_before_feature_importance.csv')\n",
    "data = pd.read_csv('./data/j_before_feature_importance_with_datetime.csv')  # datetime 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>atmospheric_pressure</th>\n",
       "      <th>snow</th>\n",
       "      <th>outflow</th>\n",
       "      <th>input</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>week</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>lag_1h</th>\n",
       "      <th>lag_2h</th>\n",
       "      <th>lag_3h</th>\n",
       "      <th>lag_24h</th>\n",
       "      <th>lag_168h</th>\n",
       "      <th>rolling_3h_avg</th>\n",
       "      <th>rolling_6h_max</th>\n",
       "      <th>rolling_6h_avg</th>\n",
       "      <th>rolling_24h_max</th>\n",
       "      <th>change_rate_1h</th>\n",
       "      <th>rolling_7d_std</th>\n",
       "      <th>delta_24h</th>\n",
       "      <th>daily_range_lag_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>6869.42</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8426.26</td>\n",
       "      <td>8376.72</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>8655.74</td>\n",
       "      <td>4672.98</td>\n",
       "      <td>8386.82</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>11937.76</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>-8.16e-03</td>\n",
       "      <td>2186.71</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>8803.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1016.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7647.39</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>8426.26</td>\n",
       "      <td>8376.72</td>\n",
       "      <td>8966.82</td>\n",
       "      <td>6425.49</td>\n",
       "      <td>8470.91</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>11147.91</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>3.25e-02</td>\n",
       "      <td>2175.26</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1017.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6164.87</td>\n",
       "      <td>4265.54</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>8426.26</td>\n",
       "      <td>8643.03</td>\n",
       "      <td>8620.11</td>\n",
       "      <td>7717.11</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>9417.22</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>-2.86e-01</td>\n",
       "      <td>2188.92</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1018.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5610.79</td>\n",
       "      <td>4194.09</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6164.87</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>8104.00</td>\n",
       "      <td>8697.04</td>\n",
       "      <td>6801.55</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7594.18</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>-8.99e-02</td>\n",
       "      <td>2207.83</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1018.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>4245.52</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5610.79</td>\n",
       "      <td>6164.87</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7819.86</td>\n",
       "      <td>9543.70</td>\n",
       "      <td>5796.57</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7133.74</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>5.84e-04</td>\n",
       "      <td>2226.98</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     temperature  precipitation  wind_speed  humidity  atmospheric_pressure  \\\n",
       "168         -0.9            0.0         1.7      78.0                1016.8   \n",
       "169         -1.0            0.0         0.9      78.0                1016.9   \n",
       "170         -1.5            0.0         1.0      80.0                1017.4   \n",
       "171         -1.7            0.0         1.6      81.0                1018.3   \n",
       "172         -1.9            0.0         1.5      81.0                1018.2   \n",
       "\n",
       "     snow  outflow    input  dayofmonth  dayofweek  quarter  month  year  \\\n",
       "168   0.0  8357.48  6869.42           8          6        1      1  2023   \n",
       "169   0.0  8628.98  7647.39           8          6        1      1  2023   \n",
       "170   0.0  6164.87  4265.54           8          6        1      1  2023   \n",
       "171   0.0  5610.79  4194.09           8          6        1      1  2023   \n",
       "172   0.0  5614.06  4245.52           8          6        1      1  2023   \n",
       "\n",
       "     dayofyear  week  hour  minute  second  is_weekend  is_holiday   lag_1h  \\\n",
       "168          8     1     0       0       0           1           1  8426.26   \n",
       "169          8     1     1       0       0           1           1  8357.48   \n",
       "170          8     1     2       0       0           1           1  8628.98   \n",
       "171          8     1     3       0       0           1           1  6164.87   \n",
       "172          8     1     4       0       0           1           1  5610.79   \n",
       "\n",
       "      lag_2h    lag_3h  lag_24h  lag_168h  rolling_3h_avg  rolling_6h_max  \\\n",
       "168  8376.72  16549.00  8655.74   4672.98         8386.82        16549.00   \n",
       "169  8426.26   8376.72  8966.82   6425.49         8470.91        16549.00   \n",
       "170  8357.48   8426.26  8643.03   8620.11         7717.11        16549.00   \n",
       "171  8628.98   8357.48  8104.00   8697.04         6801.55         8628.98   \n",
       "172  6164.87   8628.98  7819.86   9543.70         5796.57         8628.98   \n",
       "\n",
       "     rolling_6h_avg  rolling_24h_max  change_rate_1h  rolling_7d_std  \\\n",
       "168        11937.76          16549.0       -8.16e-03         2186.71   \n",
       "169        11147.91          16549.0        3.25e-02         2175.26   \n",
       "170         9417.22          16549.0       -2.86e-01         2188.92   \n",
       "171         7594.18          16549.0       -8.99e-02         2207.83   \n",
       "172         7133.74          16549.0        5.84e-04         2226.98   \n",
       "\n",
       "     delta_24h  daily_range_lag_1d  \n",
       "168      -0.03             8803.18  \n",
       "169      -0.04             7377.15  \n",
       "170      -0.29             7377.15  \n",
       "171      -0.31             7377.15  \n",
       "172      -0.28             7377.15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.reset_index(drop=True), df['outflow'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>atmospheric_pressure</th>\n",
       "      <th>snow</th>\n",
       "      <th>outflow</th>\n",
       "      <th>input</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>week</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>lag_1h</th>\n",
       "      <th>lag_2h</th>\n",
       "      <th>lag_3h</th>\n",
       "      <th>lag_24h</th>\n",
       "      <th>lag_168h</th>\n",
       "      <th>rolling_3h_avg</th>\n",
       "      <th>rolling_6h_max</th>\n",
       "      <th>rolling_6h_avg</th>\n",
       "      <th>rolling_24h_max</th>\n",
       "      <th>change_rate_1h</th>\n",
       "      <th>rolling_7d_std</th>\n",
       "      <th>delta_24h</th>\n",
       "      <th>daily_range_lag_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>6869.42</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8426.26</td>\n",
       "      <td>8376.72</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>8655.74</td>\n",
       "      <td>4672.98</td>\n",
       "      <td>8386.82</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>11937.76</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>-8.16e-03</td>\n",
       "      <td>2186.71</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>8803.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1016.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7647.39</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>8426.26</td>\n",
       "      <td>8376.72</td>\n",
       "      <td>8966.82</td>\n",
       "      <td>6425.49</td>\n",
       "      <td>8470.91</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>11147.91</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>3.25e-02</td>\n",
       "      <td>2175.26</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1017.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6164.87</td>\n",
       "      <td>4265.54</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>8426.26</td>\n",
       "      <td>8643.03</td>\n",
       "      <td>8620.11</td>\n",
       "      <td>7717.11</td>\n",
       "      <td>16549.00</td>\n",
       "      <td>9417.22</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>-2.86e-01</td>\n",
       "      <td>2188.92</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1018.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5610.79</td>\n",
       "      <td>4194.09</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6164.87</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>8357.48</td>\n",
       "      <td>8104.00</td>\n",
       "      <td>8697.04</td>\n",
       "      <td>6801.55</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7594.18</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>-8.99e-02</td>\n",
       "      <td>2207.83</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1018.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>4245.52</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5610.79</td>\n",
       "      <td>6164.87</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7819.86</td>\n",
       "      <td>9543.70</td>\n",
       "      <td>5796.57</td>\n",
       "      <td>8628.98</td>\n",
       "      <td>7133.74</td>\n",
       "      <td>16549.0</td>\n",
       "      <td>5.84e-04</td>\n",
       "      <td>2226.98</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>7377.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  precipitation  wind_speed  humidity  atmospheric_pressure  \\\n",
       "0         -0.9            0.0         1.7      78.0                1016.8   \n",
       "1         -1.0            0.0         0.9      78.0                1016.9   \n",
       "2         -1.5            0.0         1.0      80.0                1017.4   \n",
       "3         -1.7            0.0         1.6      81.0                1018.3   \n",
       "4         -1.9            0.0         1.5      81.0                1018.2   \n",
       "\n",
       "   snow  outflow    input  dayofmonth  dayofweek  quarter  month  year  \\\n",
       "0   0.0  8357.48  6869.42           8          6        1      1  2023   \n",
       "1   0.0  8628.98  7647.39           8          6        1      1  2023   \n",
       "2   0.0  6164.87  4265.54           8          6        1      1  2023   \n",
       "3   0.0  5610.79  4194.09           8          6        1      1  2023   \n",
       "4   0.0  5614.06  4245.52           8          6        1      1  2023   \n",
       "\n",
       "   dayofyear  week  hour  minute  second  is_weekend  is_holiday   lag_1h  \\\n",
       "0          8     1     0       0       0           1           1  8426.26   \n",
       "1          8     1     1       0       0           1           1  8357.48   \n",
       "2          8     1     2       0       0           1           1  8628.98   \n",
       "3          8     1     3       0       0           1           1  6164.87   \n",
       "4          8     1     4       0       0           1           1  5610.79   \n",
       "\n",
       "    lag_2h    lag_3h  lag_24h  lag_168h  rolling_3h_avg  rolling_6h_max  \\\n",
       "0  8376.72  16549.00  8655.74   4672.98         8386.82        16549.00   \n",
       "1  8426.26   8376.72  8966.82   6425.49         8470.91        16549.00   \n",
       "2  8357.48   8426.26  8643.03   8620.11         7717.11        16549.00   \n",
       "3  8628.98   8357.48  8104.00   8697.04         6801.55         8628.98   \n",
       "4  6164.87   8628.98  7819.86   9543.70         5796.57         8628.98   \n",
       "\n",
       "   rolling_6h_avg  rolling_24h_max  change_rate_1h  rolling_7d_std  delta_24h  \\\n",
       "0        11937.76          16549.0       -8.16e-03         2186.71      -0.03   \n",
       "1        11147.91          16549.0        3.25e-02         2175.26      -0.04   \n",
       "2         9417.22          16549.0       -2.86e-01         2188.92      -0.29   \n",
       "3         7594.18          16549.0       -8.99e-02         2207.83      -0.31   \n",
       "4         7133.74          16549.0        5.84e-04         2226.98      -0.28   \n",
       "\n",
       "   daily_range_lag_1d  \n",
       "0             8803.18  \n",
       "1             7377.15  \n",
       "2             7377.15  \n",
       "3             7377.15  \n",
       "4             7377.15  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.7)\n",
    "\n",
    "trainset_feature = X[:train_size]\n",
    "trainset_target = y[:train_size]\n",
    "testset_feature = X[train_size:]\n",
    "testset_target = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_feature = MinMaxScaler()\n",
    "train_feature_scaled = scaler_feature.fit_transform(trainset_feature)\n",
    "test_feature_scaled = scaler_feature.transform(testset_feature)\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "train_target_scaled = scaler_target.fit_transform(trainset_target)\n",
    "test_target_scaled = scaler_target.transform(testset_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(train_feature_scaled.shape[0] * 0.9)\n",
    "\n",
    "train_feature_scale = train_feature_scaled[:valid_size]\n",
    "train_target_scale = train_target_scaled[:valid_size]\n",
    "valid_feature_scaled = train_feature_scaled[valid_size:]\n",
    "valid_target_scaled = train_target_scaled[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('scaler_feature_lstm_v10.pkl', 'wb') as f:\n",
    "  pickle.dump(scaler_feature, f)\n",
    "with open('scaler_target_lstm_v10.pkl', 'wb') as f:\n",
    "  pickle.dump(scaler_target, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, target, seq_len, pred_len):\n",
    "  X, y = [], []\n",
    "  for i in range(data.shape[0] - (seq_len + pred_len) + 1):\n",
    "    X.append(data[i:i + seq_len, :])\n",
    "    y.append(target[i + seq_len:i + seq_len + pred_len])\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10580, 168, 33) (907, 168, 33) (4439, 168, 33) (10580, 1, 1) (907, 1, 1) (4439, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 168  # 과거 7일 (시간 단위)\n",
    "pred_len = 1  # 이후 1시간 예측\n",
    "\n",
    "X_train, y_train = split_dataset(train_feature_scaled, train_target_scaled, seq_len, pred_len)\n",
    "X_valid, y_valid = split_dataset(valid_feature_scaled, valid_target_scaled, seq_len, pred_len)\n",
    "X_test, y_test = split_dataset(test_feature_scaled, test_target_scaled, seq_len, pred_len)\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.data = torch.tensor(X, dtype=torch.float32)\n",
    "    self.labels = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)  # 데이터의 길이를 반환\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx], self.labels[idx] # 데이터와 레이블을 함께 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "valid_dataset = SequenceDataset(X_valid, y_valid)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(33, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    lstm_out, _ = self.lstm(x)\n",
    "    last_out = lstm_out[:, -1, :]\n",
    "    out = self.fc(last_out)\n",
    "    return out\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = TARGET_LENGTH\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_dataset, loss_fn, device):\n",
    "  val_loss = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for X, y in valid_dataset:\n",
    "      X_batch, y_batch = X.to(device), y.to(device)\n",
    "      out = model(X_batch)\n",
    "      loss = loss_fn(out, y_batch)\n",
    "      val_loss += loss.item()\n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([128, 1, 1])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([84, 1, 1])) that is different to the input size (torch.Size([84, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([11, 1, 1])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 300\n",
      "Train Loss : 2.6842675302177668\n",
      "Valid Loss : 0.14814091101288795\n",
      "Epoch : 2 / 300\n",
      "Train Loss : 1.6871117819100618\n",
      "Valid Loss : 0.14710033684968948\n",
      "Epoch : 3 / 300\n",
      "Train Loss : 1.646340380422771\n",
      "Valid Loss : 0.14670425094664097\n",
      "Epoch : 4 / 300\n",
      "Train Loss : 1.6364772012457252\n",
      "Valid Loss : 0.14514208398759365\n",
      "Epoch : 5 / 300\n",
      "Train Loss : 1.6318302396684885\n",
      "Valid Loss : 0.14546384289860725\n",
      "Epoch : 6 / 300\n",
      "Train Loss : 1.6291987746953964\n",
      "Valid Loss : 0.14536708500236273\n",
      "Epoch : 7 / 300\n",
      "Train Loss : 1.6488486165180802\n",
      "Valid Loss : 0.14427818823605776\n",
      "Epoch : 8 / 300\n",
      "Train Loss : 1.632421476766467\n",
      "Valid Loss : 0.14683786127716303\n",
      "Epoch : 9 / 300\n",
      "Train Loss : 1.6400932352989912\n",
      "Valid Loss : 0.14426002837717533\n",
      "Epoch : 10 / 300\n",
      "Train Loss : 1.6364300902932882\n",
      "Valid Loss : 0.1442729691043496\n",
      "Epoch : 11 / 300\n",
      "Train Loss : 1.6414349023252726\n",
      "Valid Loss : 0.14409924484789371\n",
      "Epoch : 12 / 300\n",
      "Train Loss : 1.6290398798882961\n",
      "Valid Loss : 0.14475528243929148\n",
      "Epoch : 13 / 300\n",
      "Train Loss : 1.6418271735310555\n",
      "Valid Loss : 0.14904404990375042\n",
      "Epoch : 14 / 300\n",
      "Train Loss : 1.632548806257546\n",
      "Valid Loss : 0.14407350588589907\n",
      "Epoch : 15 / 300\n",
      "Train Loss : 1.6282729348167777\n",
      "Valid Loss : 0.1461169794201851\n",
      "Epoch : 16 / 300\n",
      "Train Loss : 1.627193046733737\n",
      "Valid Loss : 0.14386504329741\n",
      "Epoch : 17 / 300\n",
      "Train Loss : 1.6309510711580515\n",
      "Valid Loss : 0.14412555191665888\n",
      "Epoch : 18 / 300\n",
      "Train Loss : 1.6294930102303624\n",
      "Valid Loss : 0.14498269092291594\n",
      "Epoch : 19 / 300\n",
      "Train Loss : 1.6277766423299909\n",
      "Valid Loss : 0.1448871474713087\n",
      "Epoch : 20 / 300\n",
      "Train Loss : 1.6250634845346212\n",
      "Valid Loss : 0.14387078769505024\n",
      "Epoch : 21 / 300\n",
      "Train Loss : 1.6346786208450794\n",
      "Valid Loss : 0.14767608232796192\n",
      "Epoch : 22 / 300\n",
      "Train Loss : 1.6337929833680391\n",
      "Valid Loss : 0.1439523035660386\n",
      "Epoch : 23 / 300\n",
      "Train Loss : 1.6303252205252647\n",
      "Valid Loss : 0.1441387813538313\n",
      "Epoch : 24 / 300\n",
      "Train Loss : 1.6287516010925174\n",
      "Valid Loss : 0.1456654341891408\n",
      "Epoch : 25 / 300\n",
      "Train Loss : 1.6288705430924892\n",
      "Valid Loss : 0.14869552105665207\n",
      "Epoch : 26 / 300\n",
      "Train Loss : 1.6280760755762458\n",
      "Valid Loss : 0.1451963633298874\n",
      "Epoch : 27 / 300\n",
      "Train Loss : 1.6471612891182303\n",
      "Valid Loss : 0.14887871500104666\n",
      "Epoch : 28 / 300\n",
      "Train Loss : 1.6286533679813147\n",
      "Valid Loss : 0.1441226815804839\n",
      "Epoch : 29 / 300\n",
      "Train Loss : 1.6234501618891954\n",
      "Valid Loss : 0.14379409980028868\n",
      "Epoch : 30 / 300\n",
      "Train Loss : 1.626783867366612\n",
      "Valid Loss : 0.1437802603468299\n",
      "Epoch : 31 / 300\n",
      "Train Loss : 1.6293344963341951\n",
      "Valid Loss : 0.14435885194689035\n",
      "Epoch : 32 / 300\n",
      "Train Loss : 1.6255870396271348\n",
      "Valid Loss : 0.14389706403017044\n",
      "Epoch : 33 / 300\n",
      "Train Loss : 1.6287397714331746\n",
      "Valid Loss : 0.1442378107458353\n",
      "Epoch : 34 / 300\n",
      "Train Loss : 1.6283661667257547\n",
      "Valid Loss : 0.14787550549954176\n",
      "Epoch : 35 / 300\n",
      "Train Loss : 1.6336401347070932\n",
      "Valid Loss : 0.14380877371877432\n",
      "Epoch : 36 / 300\n",
      "Train Loss : 1.6265456015244126\n",
      "Valid Loss : 0.1436714306473732\n",
      "Epoch : 37 / 300\n",
      "Train Loss : 1.6273476919159293\n",
      "Valid Loss : 0.14450856391340494\n",
      "Epoch : 38 / 300\n",
      "Train Loss : 1.6267569866031408\n",
      "Valid Loss : 0.1440969668328762\n",
      "Epoch : 39 / 300\n",
      "Train Loss : 1.6301292683929205\n",
      "Valid Loss : 0.14405982103198767\n",
      "Epoch : 40 / 300\n",
      "Train Loss : 1.6231346242129803\n",
      "Valid Loss : 0.1441855262964964\n",
      "Epoch : 41 / 300\n",
      "Train Loss : 1.6248345552012324\n",
      "Valid Loss : 0.14362172968685627\n",
      "Epoch : 42 / 300\n",
      "Train Loss : 1.6301850331947207\n",
      "Valid Loss : 0.14375865645706654\n",
      "Epoch : 43 / 300\n",
      "Train Loss : 1.6254119891673326\n",
      "Valid Loss : 0.14467986952513456\n",
      "Epoch : 44 / 300\n",
      "Train Loss : 1.628567866049707\n",
      "Valid Loss : 0.14351187832653522\n",
      "Epoch : 45 / 300\n",
      "Train Loss : 1.6352453734725714\n",
      "Valid Loss : 0.14819164387881756\n",
      "Epoch : 46 / 300\n",
      "Train Loss : 1.630394373089075\n",
      "Valid Loss : 0.14444023743271828\n",
      "Epoch : 47 / 300\n",
      "Train Loss : 1.635548947378993\n",
      "Valid Loss : 0.14372980780899525\n",
      "Epoch : 48 / 300\n",
      "Train Loss : 1.6289789834991097\n",
      "Valid Loss : 0.1439522933214903\n",
      "Epoch : 49 / 300\n",
      "Train Loss : 1.6240361202508211\n",
      "Valid Loss : 0.14433487318456173\n",
      "Epoch : 50 / 300\n",
      "Train Loss : 1.6250846022740006\n",
      "Valid Loss : 0.14583236165344715\n",
      "Epoch : 51 / 300\n",
      "Train Loss : 1.627680690959096\n",
      "Valid Loss : 0.14416614640504122\n",
      "Epoch : 52 / 300\n",
      "Train Loss : 1.6307238982990384\n",
      "Valid Loss : 0.14518992602825165\n",
      "Epoch : 53 / 300\n",
      "Train Loss : 1.630506332963705\n",
      "Valid Loss : 0.14520746190100908\n",
      "Epoch : 54 / 300\n",
      "Train Loss : 1.6308844117447734\n",
      "Valid Loss : 0.14366552606225014\n",
      "Epoch : 55 / 300\n",
      "Train Loss : 1.6318345936015248\n",
      "Valid Loss : 0.1444719722494483\n",
      "Epoch : 56 / 300\n",
      "Train Loss : 1.634712690487504\n",
      "Valid Loss : 0.146146347746253\n",
      "Epoch : 57 / 300\n",
      "Train Loss : 1.6294401921331882\n",
      "Valid Loss : 0.1436536880210042\n",
      "Epoch : 58 / 300\n",
      "Train Loss : 1.6295372880995274\n",
      "Valid Loss : 0.1436738781630993\n",
      "Epoch : 59 / 300\n",
      "Train Loss : 1.6321387328207493\n",
      "Valid Loss : 0.1444255094975233\n",
      "Epoch : 60 / 300\n",
      "Train Loss : 1.6269539659842849\n",
      "Valid Loss : 0.1435284437611699\n",
      "Epoch : 61 / 300\n",
      "Train Loss : 1.6259580850601196\n",
      "Valid Loss : 0.14376459736377\n",
      "Epoch : 62 / 300\n",
      "Train Loss : 1.6240404406562448\n",
      "Valid Loss : 0.1435852376744151\n",
      "Epoch : 63 / 300\n",
      "Train Loss : 1.625403224490583\n",
      "Valid Loss : 0.14429567754268646\n",
      "Epoch : 64 / 300\n",
      "Train Loss : 1.6315055256709456\n",
      "Valid Loss : 0.14404012728482485\n",
      "Epoch : 65 / 300\n",
      "Train Loss : 1.6356075638905168\n",
      "Valid Loss : 0.1446178313344717\n",
      "Epoch : 66 / 300\n",
      "Train Loss : 1.630310789681971\n",
      "Valid Loss : 0.14425927493721247\n",
      "Epoch : 67 / 300\n",
      "Train Loss : 1.6289437878876925\n",
      "Valid Loss : 0.14371036365628242\n",
      "Epoch : 68 / 300\n",
      "Train Loss : 1.632473218254745\n",
      "Valid Loss : 0.14381648786365986\n",
      "Epoch : 69 / 300\n",
      "Train Loss : 1.6265103397890925\n",
      "Valid Loss : 0.14387044683098793\n",
      "Epoch : 70 / 300\n",
      "Train Loss : 1.629046257585287\n",
      "Valid Loss : 0.14500327222049236\n",
      "Epoch : 71 / 300\n",
      "Train Loss : 1.6281480817124248\n",
      "Valid Loss : 0.14356869272887707\n",
      "Epoch : 72 / 300\n",
      "Train Loss : 1.6253647664561868\n",
      "Valid Loss : 0.14350333344191313\n",
      "Epoch : 73 / 300\n",
      "Train Loss : 1.6254025753587484\n",
      "Valid Loss : 0.14355958439409733\n",
      "Epoch : 74 / 300\n",
      "Train Loss : 1.6229703789576888\n",
      "Valid Loss : 0.1459440141916275\n",
      "Epoch : 75 / 300\n",
      "Train Loss : 1.6250368412584066\n",
      "Valid Loss : 0.14370013680309057\n",
      "Epoch : 76 / 300\n",
      "Train Loss : 1.6263103820383549\n",
      "Valid Loss : 0.14351951889693737\n",
      "Epoch : 77 / 300\n",
      "Train Loss : 1.622863688506186\n",
      "Valid Loss : 0.1465066997334361\n",
      "Epoch : 78 / 300\n",
      "Train Loss : 1.6290537416934967\n",
      "Valid Loss : 0.14390869159251451\n",
      "Epoch : 79 / 300\n",
      "Train Loss : 1.6266091018915176\n",
      "Valid Loss : 0.14360538590699434\n",
      "Epoch : 80 / 300\n",
      "Train Loss : 1.6233267579227686\n",
      "Valid Loss : 0.1439422545954585\n",
      "Epoch : 81 / 300\n",
      "Train Loss : 1.6302332691848278\n",
      "Valid Loss : 0.14410264510661364\n",
      "Epoch : 82 / 300\n",
      "Train Loss : 1.6239482322707772\n",
      "Valid Loss : 0.14459581207484007\n",
      "Epoch : 83 / 300\n",
      "Train Loss : 1.6249638488516212\n",
      "Valid Loss : 0.14396587014198303\n",
      "Epoch : 84 / 300\n",
      "Train Loss : 1.6310354098677635\n",
      "Valid Loss : 0.1436017733067274\n",
      "Epoch : 85 / 300\n",
      "Train Loss : 1.6270702881738544\n",
      "Valid Loss : 0.14370651170611382\n",
      "Epoch : 86 / 300\n",
      "Train Loss : 1.627932663075626\n",
      "Valid Loss : 0.1449980791658163\n",
      "Epoch : 87 / 300\n",
      "Train Loss : 1.6242300886660814\n",
      "Valid Loss : 0.14364434499293566\n",
      "Epoch : 88 / 300\n",
      "Train Loss : 1.629727965220809\n",
      "Valid Loss : 0.14397412724792957\n",
      "Epoch : 89 / 300\n",
      "Train Loss : 1.629410993307829\n",
      "Valid Loss : 0.14446912705898285\n",
      "Epoch : 90 / 300\n",
      "Train Loss : 1.6258888561278582\n",
      "Valid Loss : 0.14383127074688673\n",
      "Epoch : 91 / 300\n",
      "Train Loss : 1.6283197049051523\n",
      "Valid Loss : 0.14356643799692392\n",
      "Epoch : 92 / 300\n",
      "Train Loss : 1.6262904610484838\n",
      "Valid Loss : 0.14873921032994986\n",
      "Epoch : 93 / 300\n",
      "Train Loss : 1.6263150162994862\n",
      "Valid Loss : 0.14337326306849718\n",
      "Epoch : 94 / 300\n",
      "Train Loss : 1.628034669905901\n",
      "Valid Loss : 0.1443786546587944\n",
      "Epoch : 95 / 300\n",
      "Train Loss : 1.6242518294602633\n",
      "Valid Loss : 0.1446636924520135\n",
      "Epoch : 96 / 300\n",
      "Train Loss : 1.6263087335973978\n",
      "Valid Loss : 0.14462969079613686\n",
      "Epoch : 97 / 300\n",
      "Train Loss : 1.6257932037115097\n",
      "Valid Loss : 0.14339128881692886\n",
      "Epoch : 98 / 300\n",
      "Train Loss : 1.623681716620922\n",
      "Valid Loss : 0.14416837971657515\n",
      "Epoch : 99 / 300\n",
      "Train Loss : 1.6289289593696594\n",
      "Valid Loss : 0.14371615462005138\n",
      "Epoch : 100 / 300\n",
      "Train Loss : 1.6238910974934697\n",
      "Valid Loss : 0.1435502953827381\n",
      "Epoch : 101 / 300\n",
      "Train Loss : 1.6250021094456315\n",
      "Valid Loss : 0.14366480521857738\n",
      "Epoch : 102 / 300\n",
      "Train Loss : 1.6303439103066921\n",
      "Valid Loss : 0.1434561824426055\n",
      "Epoch : 103 / 300\n",
      "Train Loss : 1.633691107854247\n",
      "Valid Loss : 0.14443805534392595\n",
      "Epoch : 104 / 300\n",
      "Train Loss : 1.6295241815969348\n",
      "Valid Loss : 0.14376889448612928\n",
      "Epoch : 105 / 300\n",
      "Train Loss : 1.6252866312861443\n",
      "Valid Loss : 0.14413850475102663\n",
      "Epoch : 106 / 300\n",
      "Train Loss : 1.6252633295953274\n",
      "Valid Loss : 0.14379767142236233\n",
      "Epoch : 107 / 300\n",
      "Train Loss : 1.626174457371235\n",
      "Valid Loss : 0.14400192257016897\n",
      "Epoch : 108 / 300\n",
      "Train Loss : 1.6278108954429626\n",
      "Valid Loss : 0.14360905345529318\n",
      "Epoch : 109 / 300\n",
      "Train Loss : 1.624601791612804\n",
      "Valid Loss : 0.14506241120398045\n",
      "Epoch : 110 / 300\n",
      "Train Loss : 1.6263824021443725\n",
      "Valid Loss : 0.14335777889937162\n",
      "Epoch : 111 / 300\n",
      "Train Loss : 1.6255380297079682\n",
      "Valid Loss : 0.14361341763287783\n",
      "Epoch : 112 / 300\n",
      "Train Loss : 1.6259806882590055\n",
      "Valid Loss : 0.14412686973810196\n",
      "Epoch : 113 / 300\n",
      "Train Loss : 1.6234680339694023\n",
      "Valid Loss : 0.1434482354670763\n",
      "Epoch : 114 / 300\n",
      "Train Loss : 1.6229907851666212\n",
      "Valid Loss : 0.1433634301647544\n",
      "Epoch : 115 / 300\n",
      "Train Loss : 1.6238287072628736\n",
      "Valid Loss : 0.143489520996809\n",
      "Epoch : 116 / 300\n",
      "Train Loss : 1.6274716220796108\n",
      "Valid Loss : 0.1469039088115096\n",
      "Epoch : 117 / 300\n",
      "Train Loss : 1.6359772123396397\n",
      "Valid Loss : 0.14479197561740875\n",
      "Epoch : 118 / 300\n",
      "Train Loss : 1.6270329989492893\n",
      "Valid Loss : 0.14563938695937395\n",
      "Epoch : 119 / 300\n",
      "Train Loss : 1.627930344082415\n",
      "Valid Loss : 0.1434914330020547\n",
      "Epoch : 120 / 300\n",
      "Train Loss : 1.6223585465922952\n",
      "Valid Loss : 0.1435037376359105\n",
      "Epoch : 121 / 300\n",
      "Train Loss : 1.6233407463878393\n",
      "Valid Loss : 0.14636621437966824\n",
      "Epoch : 122 / 300\n",
      "Train Loss : 1.6263362113386393\n",
      "Valid Loss : 0.14340825006365776\n",
      "Epoch : 123 / 300\n",
      "Train Loss : 1.6233472591266036\n",
      "Valid Loss : 0.14339633751660585\n",
      "Epoch : 124 / 300\n",
      "Train Loss : 1.6238707518205047\n",
      "Valid Loss : 0.14351131860166788\n",
      "Epoch : 125 / 300\n",
      "Train Loss : 1.6261187084019184\n",
      "Valid Loss : 0.14350597094744444\n",
      "Epoch : 126 / 300\n",
      "Train Loss : 1.6262391321361065\n",
      "Valid Loss : 0.14487732760608196\n",
      "Epoch : 127 / 300\n",
      "Train Loss : 1.6283258637413383\n",
      "Valid Loss : 0.14352076314389706\n",
      "Epoch : 128 / 300\n",
      "Train Loss : 1.627548910677433\n",
      "Valid Loss : 0.144996028393507\n",
      "Epoch : 129 / 300\n",
      "Train Loss : 1.6238302765414119\n",
      "Valid Loss : 0.14371750317513943\n",
      "Epoch : 130 / 300\n",
      "Train Loss : 1.625752910040319\n",
      "Valid Loss : 0.14457221701741219\n",
      "Epoch : 131 / 300\n",
      "Train Loss : 1.626366337761283\n",
      "Valid Loss : 0.1443940680474043\n",
      "Epoch : 132 / 300\n",
      "Train Loss : 1.627860875800252\n",
      "Valid Loss : 0.14346740301698446\n",
      "Epoch : 133 / 300\n",
      "Train Loss : 1.6248973282054067\n",
      "Valid Loss : 0.14363995287567377\n",
      "Epoch : 134 / 300\n",
      "Train Loss : 1.6253965301439166\n",
      "Valid Loss : 0.14362558256834745\n",
      "Epoch : 135 / 300\n",
      "Train Loss : 1.6259728455916047\n",
      "Valid Loss : 0.14341159909963608\n",
      "Epoch : 136 / 300\n",
      "Train Loss : 1.629666954278946\n",
      "Valid Loss : 0.14568537287414074\n",
      "Epoch : 137 / 300\n",
      "Train Loss : 1.6263982132077217\n",
      "Valid Loss : 0.1440833667293191\n",
      "Epoch : 138 / 300\n",
      "Train Loss : 1.6282980302348733\n",
      "Valid Loss : 0.14335884433239698\n",
      "Epoch : 139 / 300\n",
      "Train Loss : 1.6227633468806744\n",
      "Valid Loss : 0.1434019124135375\n",
      "Epoch : 140 / 300\n",
      "Train Loss : 1.6278277281671762\n",
      "Valid Loss : 0.14384234510362148\n",
      "Epoch : 141 / 300\n",
      "Train Loss : 1.6229783995077014\n",
      "Valid Loss : 0.14370405860245228\n",
      "Epoch : 142 / 300\n",
      "Train Loss : 1.6254799235612154\n",
      "Valid Loss : 0.14454687386751175\n",
      "Epoch : 143 / 300\n",
      "Train Loss : 1.6232473645359278\n",
      "Valid Loss : 0.14410137571394444\n",
      "Epoch : 144 / 300\n",
      "Train Loss : 1.624500005505979\n",
      "Valid Loss : 0.1461417330428958\n",
      "Epoch : 145 / 300\n",
      "Train Loss : 1.6292002107948065\n",
      "Valid Loss : 0.14404217898845673\n",
      "Epoch : 146 / 300\n",
      "Train Loss : 1.6244676755741239\n",
      "Valid Loss : 0.14391665905714035\n",
      "Epoch : 147 / 300\n",
      "Train Loss : 1.624371905811131\n",
      "Valid Loss : 0.14400575403124094\n",
      "Epoch : 148 / 300\n",
      "Train Loss : 1.6255664769560099\n",
      "Valid Loss : 0.14840561337769032\n",
      "Epoch : 149 / 300\n",
      "Train Loss : 1.6250826735049486\n",
      "Valid Loss : 0.14377191569656134\n",
      "Epoch : 150 / 300\n",
      "Train Loss : 1.6266132909804583\n",
      "Valid Loss : 0.14352370146661997\n",
      "Epoch : 151 / 300\n",
      "Train Loss : 1.6252053584903479\n",
      "Valid Loss : 0.14340672548860312\n",
      "Epoch : 152 / 300\n",
      "Train Loss : 1.62535591237247\n",
      "Valid Loss : 0.14471589867025614\n",
      "Epoch : 153 / 300\n",
      "Train Loss : 1.6308782305568457\n",
      "Valid Loss : 0.14459482859820127\n",
      "Epoch : 154 / 300\n",
      "Train Loss : 1.625362154096365\n",
      "Valid Loss : 0.1438571074977517\n",
      "Epoch : 155 / 300\n",
      "Train Loss : 1.626014318317175\n",
      "Valid Loss : 0.14352795481681824\n",
      "Epoch : 156 / 300\n",
      "Train Loss : 1.6276187235489488\n",
      "Valid Loss : 0.1434020372107625\n",
      "Epoch : 157 / 300\n",
      "Train Loss : 1.6228671930730343\n",
      "Valid Loss : 0.14334507286548615\n",
      "Epoch : 158 / 300\n",
      "Train Loss : 1.624155160970986\n",
      "Valid Loss : 0.14340054523199797\n",
      "Epoch : 159 / 300\n",
      "Train Loss : 1.6256242906674743\n",
      "Valid Loss : 0.14447723422199488\n",
      "Epoch : 160 / 300\n",
      "Train Loss : 1.6261791968718171\n",
      "Valid Loss : 0.14420214667916298\n",
      "Epoch : 161 / 300\n",
      "Train Loss : 1.6226880680769682\n",
      "Valid Loss : 0.1434230711311102\n",
      "Epoch : 162 / 300\n",
      "Train Loss : 1.6231523184105754\n",
      "Valid Loss : 0.14332198165357113\n",
      "Epoch : 163 / 300\n",
      "Train Loss : 1.6276669865474105\n",
      "Valid Loss : 0.14378952700644732\n",
      "Epoch : 164 / 300\n",
      "Train Loss : 1.6246891738846898\n",
      "Valid Loss : 0.1433744989335537\n",
      "Epoch : 165 / 300\n",
      "Train Loss : 1.6244520004838705\n",
      "Valid Loss : 0.14396996889263391\n",
      "Epoch : 166 / 300\n",
      "Train Loss : 1.624805236235261\n",
      "Valid Loss : 0.14352973084896803\n",
      "Epoch : 167 / 300\n",
      "Train Loss : 1.6272514257580042\n",
      "Valid Loss : 0.1441507562994957\n",
      "Epoch : 168 / 300\n",
      "Train Loss : 1.626278355717659\n",
      "Valid Loss : 0.1467015417292714\n",
      "Epoch : 169 / 300\n",
      "Train Loss : 1.6256601801142097\n",
      "Valid Loss : 0.1437978446483612\n",
      "Epoch : 170 / 300\n",
      "Train Loss : 1.6225932221859694\n",
      "Valid Loss : 0.14418545830994844\n",
      "Epoch : 171 / 300\n",
      "Train Loss : 1.6251339949667454\n",
      "Valid Loss : 0.14351830631494522\n",
      "Epoch : 172 / 300\n",
      "Train Loss : 1.6254191314801574\n",
      "Valid Loss : 0.14340346306562424\n",
      "Epoch : 173 / 300\n",
      "Train Loss : 1.6242289943620563\n",
      "Valid Loss : 0.14436677005141973\n",
      "Epoch : 174 / 300\n",
      "Train Loss : 1.6245484231039882\n",
      "Valid Loss : 0.14343949314206839\n",
      "Epoch : 175 / 300\n",
      "Train Loss : 1.6238369243219495\n",
      "Valid Loss : 0.14435049425810575\n",
      "Epoch : 176 / 300\n",
      "Train Loss : 1.6258822623640299\n",
      "Valid Loss : 0.14356164075434208\n",
      "Epoch : 177 / 300\n",
      "Train Loss : 1.6220773495733738\n",
      "Valid Loss : 0.14354511629790068\n",
      "Epoch : 178 / 300\n",
      "Train Loss : 1.623227016068995\n",
      "Valid Loss : 0.14378229808062315\n",
      "Epoch : 179 / 300\n",
      "Train Loss : 1.62390548735857\n",
      "Valid Loss : 0.14378129411488771\n",
      "Epoch : 180 / 300\n",
      "Train Loss : 1.6240035947412252\n",
      "Valid Loss : 0.14381320122629404\n",
      "Epoch : 181 / 300\n",
      "Train Loss : 1.6262845965102315\n",
      "Valid Loss : 0.14363843854516745\n",
      "Epoch : 182 / 300\n",
      "Train Loss : 1.6274853218346834\n",
      "Valid Loss : 0.14469661936163902\n",
      "Epoch : 183 / 300\n",
      "Train Loss : 1.6251748027279973\n",
      "Valid Loss : 0.1434851810336113\n",
      "Epoch : 184 / 300\n",
      "Train Loss : 1.6301165167242289\n",
      "Valid Loss : 0.1460531884804368\n",
      "Epoch : 185 / 300\n",
      "Train Loss : 1.6238732552155852\n",
      "Valid Loss : 0.14392587263137102\n",
      "Epoch : 186 / 300\n",
      "Train Loss : 1.624316118657589\n",
      "Valid Loss : 0.14386568777263165\n",
      "Epoch : 187 / 300\n",
      "Train Loss : 1.6279363073408604\n",
      "Valid Loss : 0.14790794905275106\n",
      "Epoch : 188 / 300\n",
      "Train Loss : 1.6251674555242062\n",
      "Valid Loss : 0.14692793413996696\n",
      "Epoch : 189 / 300\n",
      "Train Loss : 1.6265990752726793\n",
      "Valid Loss : 0.14345820806920528\n",
      "Epoch : 190 / 300\n",
      "Train Loss : 1.6248819818720222\n",
      "Valid Loss : 0.14339476078748703\n",
      "Epoch : 191 / 300\n",
      "Train Loss : 1.6242237901315093\n",
      "Valid Loss : 0.14342585857957602\n",
      "Epoch : 192 / 300\n",
      "Train Loss : 1.624805229716003\n",
      "Valid Loss : 0.14733358286321163\n",
      "Epoch : 193 / 300\n",
      "Train Loss : 1.6272685173898935\n",
      "Valid Loss : 0.1442865738645196\n",
      "Epoch : 194 / 300\n",
      "Train Loss : 1.624021004885435\n",
      "Valid Loss : 0.14393542613834143\n",
      "Epoch : 195 / 300\n",
      "Train Loss : 1.6212886180728674\n",
      "Valid Loss : 0.14338695909827948\n",
      "Epoch : 196 / 300\n",
      "Train Loss : 1.6292134886607528\n",
      "Valid Loss : 0.1433467986062169\n",
      "Epoch : 197 / 300\n",
      "Train Loss : 1.6284419130533934\n",
      "Valid Loss : 0.14458849001675844\n",
      "Epoch : 198 / 300\n",
      "Train Loss : 1.6222077682614326\n",
      "Valid Loss : 0.14367014542222023\n",
      "Epoch : 199 / 300\n",
      "Train Loss : 1.6230119401589036\n",
      "Valid Loss : 0.14423630759119987\n",
      "Epoch : 200 / 300\n",
      "Train Loss : 1.6247187983244658\n",
      "Valid Loss : 0.14399004727602005\n",
      "Epoch : 201 / 300\n",
      "Train Loss : 1.622894818894565\n",
      "Valid Loss : 0.14441481698304415\n",
      "Epoch : 202 / 300\n",
      "Train Loss : 1.6244806479662657\n",
      "Valid Loss : 0.1434585191309452\n",
      "Epoch : 203 / 300\n",
      "Train Loss : 1.6251858603209257\n",
      "Valid Loss : 0.14384180027991533\n",
      "Epoch : 204 / 300\n",
      "Train Loss : 1.6255369409918785\n",
      "Valid Loss : 0.14417719561606646\n",
      "Epoch : 205 / 300\n",
      "Train Loss : 1.6238982155919075\n",
      "Valid Loss : 0.14338395651429892\n",
      "Epoch : 206 / 300\n",
      "Train Loss : 1.6238087322562933\n",
      "Valid Loss : 0.14484694134443998\n",
      "Epoch : 207 / 300\n",
      "Train Loss : 1.6256373822689056\n",
      "Valid Loss : 0.14342536684125662\n",
      "Epoch : 208 / 300\n",
      "Train Loss : 1.6234014127403498\n",
      "Valid Loss : 0.14425397291779518\n",
      "Epoch : 209 / 300\n",
      "Train Loss : 1.6263605589047074\n",
      "Valid Loss : 0.14407415688037872\n",
      "Epoch : 210 / 300\n",
      "Train Loss : 1.6267937058582902\n",
      "Valid Loss : 0.14342868700623512\n",
      "Epoch : 211 / 300\n",
      "Train Loss : 1.626829239539802\n",
      "Valid Loss : 0.14337221439927816\n",
      "Epoch : 212 / 300\n",
      "Train Loss : 1.6230096314102411\n",
      "Valid Loss : 0.14374042209237814\n",
      "Epoch : 213 / 300\n",
      "Train Loss : 1.623535911552608\n",
      "Valid Loss : 0.14411456882953644\n",
      "Epoch : 214 / 300\n",
      "Train Loss : 1.6282165767624974\n",
      "Valid Loss : 0.1448136493563652\n",
      "Epoch : 215 / 300\n",
      "Train Loss : 1.6237339936196804\n",
      "Valid Loss : 0.14531761035323143\n",
      "Epoch : 216 / 300\n",
      "Train Loss : 1.622819078154862\n",
      "Valid Loss : 0.1437505865469575\n",
      "Epoch : 217 / 300\n",
      "Train Loss : 1.62361840903759\n",
      "Valid Loss : 0.14382758922874928\n",
      "Epoch : 218 / 300\n",
      "Train Loss : 1.6246929671615362\n",
      "Valid Loss : 0.14360674656927586\n",
      "Epoch : 219 / 300\n",
      "Train Loss : 1.6225901832804084\n",
      "Valid Loss : 0.14340840559452772\n",
      "Epoch : 220 / 300\n",
      "Train Loss : 1.6223080651834607\n",
      "Valid Loss : 0.14409701991826296\n",
      "Epoch : 221 / 300\n",
      "Train Loss : 1.6266575204208493\n",
      "Valid Loss : 0.14349064975976944\n",
      "Epoch : 222 / 300\n",
      "Train Loss : 1.623187143355608\n",
      "Valid Loss : 0.14344981033354998\n",
      "Epoch : 223 / 300\n",
      "Train Loss : 1.6245513884350657\n",
      "Valid Loss : 0.14442671835422516\n",
      "Epoch : 224 / 300\n",
      "Train Loss : 1.6239432645961642\n",
      "Valid Loss : 0.1436951057985425\n",
      "Epoch : 225 / 300\n",
      "Train Loss : 1.6230825241655111\n",
      "Valid Loss : 0.14343038387596607\n",
      "Epoch : 226 / 300\n",
      "Train Loss : 1.6227449793368578\n",
      "Valid Loss : 0.14370181877166033\n",
      "Epoch : 227 / 300\n",
      "Train Loss : 1.625596552155912\n",
      "Valid Loss : 0.14336609467864037\n",
      "Epoch : 228 / 300\n",
      "Train Loss : 1.6283416422083974\n",
      "Valid Loss : 0.14353487268090248\n",
      "Epoch : 229 / 300\n",
      "Train Loss : 1.6266715312376618\n",
      "Valid Loss : 0.14344283565878868\n",
      "Epoch : 230 / 300\n",
      "Train Loss : 1.6251362152397633\n",
      "Valid Loss : 0.14352352358400822\n",
      "Epoch : 231 / 300\n",
      "Train Loss : 1.6222569709643722\n",
      "Valid Loss : 0.1434624046087265\n",
      "Epoch : 232 / 300\n",
      "Train Loss : 1.6283079544082284\n",
      "Valid Loss : 0.1434023380279541\n",
      "Epoch : 233 / 300\n",
      "Train Loss : 1.6233957931399345\n",
      "Valid Loss : 0.14427575655281544\n",
      "Epoch : 234 / 300\n",
      "Train Loss : 1.6267134677618742\n",
      "Valid Loss : 0.1438026148825884\n",
      "Epoch : 235 / 300\n",
      "Train Loss : 1.6245127115398645\n",
      "Valid Loss : 0.1438068589195609\n",
      "Epoch : 236 / 300\n",
      "Train Loss : 1.6257834862917662\n",
      "Valid Loss : 0.14488853979855776\n",
      "Epoch : 237 / 300\n",
      "Train Loss : 1.6250617541372776\n",
      "Valid Loss : 0.14373338129371405\n",
      "Epoch : 238 / 300\n",
      "Train Loss : 1.6241961335763335\n",
      "Valid Loss : 0.14341707155108452\n",
      "Epoch : 239 / 300\n",
      "Train Loss : 1.6276711178943515\n",
      "Valid Loss : 0.1434381976723671\n",
      "Epoch : 240 / 300\n",
      "Train Loss : 1.6224738350138068\n",
      "Valid Loss : 0.14383361022919416\n",
      "Epoch : 241 / 300\n",
      "Train Loss : 1.6233696276322007\n",
      "Valid Loss : 0.14346812199801207\n",
      "Epoch : 242 / 300\n",
      "Train Loss : 1.62328686658293\n",
      "Valid Loss : 0.14342907443642616\n",
      "Epoch : 243 / 300\n",
      "Train Loss : 1.62429029494524\n",
      "Valid Loss : 0.1434882329776883\n",
      "Epoch : 244 / 300\n",
      "Train Loss : 1.6256476743146777\n",
      "Valid Loss : 0.14347665663808584\n",
      "Epoch : 245 / 300\n",
      "Train Loss : 1.6222136756405234\n",
      "Valid Loss : 0.1435790117830038\n",
      "Epoch : 246 / 300\n",
      "Train Loss : 1.625033383257687\n",
      "Valid Loss : 0.14369425736367702\n",
      "Epoch : 247 / 300\n",
      "Train Loss : 1.6237341351807117\n",
      "Valid Loss : 0.14565090090036392\n",
      "Epoch : 248 / 300\n",
      "Train Loss : 1.630596024915576\n",
      "Valid Loss : 0.14730003103613853\n",
      "Epoch : 249 / 300\n",
      "Train Loss : 1.6231337580829859\n",
      "Valid Loss : 0.14384863525629044\n",
      "Epoch : 250 / 300\n",
      "Train Loss : 1.6228771703317761\n",
      "Valid Loss : 0.14369853865355253\n",
      "Epoch : 251 / 300\n",
      "Train Loss : 1.6237542601302266\n",
      "Valid Loss : 0.1435373779386282\n",
      "Epoch : 252 / 300\n",
      "Train Loss : 1.6241486221551895\n",
      "Valid Loss : 0.14403630327433348\n",
      "Epoch : 253 / 300\n",
      "Train Loss : 1.6218153685331345\n",
      "Valid Loss : 0.14523376245051622\n",
      "Epoch : 254 / 300\n",
      "Train Loss : 1.6233693836256862\n",
      "Valid Loss : 0.14377630315721035\n",
      "Epoch : 255 / 300\n",
      "Train Loss : 1.6255993964150548\n",
      "Valid Loss : 0.1435925653204322\n",
      "Epoch : 256 / 300\n",
      "Train Loss : 1.6241153832525015\n",
      "Valid Loss : 0.14376199059188366\n",
      "Epoch : 257 / 300\n",
      "Train Loss : 1.6259924843907356\n",
      "Valid Loss : 0.14341841265559196\n",
      "Epoch : 258 / 300\n",
      "Train Loss : 1.6222977880388498\n",
      "Valid Loss : 0.14381363801658154\n",
      "Epoch : 259 / 300\n",
      "Train Loss : 1.6249212548136711\n",
      "Valid Loss : 0.14497392624616623\n",
      "Epoch : 260 / 300\n",
      "Train Loss : 1.6231067534536123\n",
      "Valid Loss : 0.14398412685841322\n",
      "Epoch : 261 / 300\n",
      "Train Loss : 1.6231270553544164\n",
      "Valid Loss : 0.1435634009540081\n",
      "Epoch : 262 / 300\n",
      "Train Loss : 1.6271333936601877\n",
      "Valid Loss : 0.1435928549617529\n",
      "Epoch : 263 / 300\n",
      "Train Loss : 1.627867616713047\n",
      "Valid Loss : 0.14337204582989216\n",
      "Epoch : 264 / 300\n",
      "Train Loss : 1.6228688806295395\n",
      "Valid Loss : 0.1438028085976839\n",
      "Epoch : 265 / 300\n",
      "Train Loss : 1.6254220828413963\n",
      "Valid Loss : 0.14585778955370188\n",
      "Epoch : 266 / 300\n",
      "Train Loss : 1.6269193291664124\n",
      "Valid Loss : 0.14462545327842236\n",
      "Epoch : 267 / 300\n",
      "Train Loss : 1.6241538859903812\n",
      "Valid Loss : 0.14347398653626442\n",
      "Epoch : 268 / 300\n",
      "Train Loss : 1.6242427229881287\n",
      "Valid Loss : 0.1451349351555109\n",
      "Epoch : 269 / 300\n",
      "Train Loss : 1.6273067314177752\n",
      "Valid Loss : 0.14437758456915617\n",
      "Epoch : 270 / 300\n",
      "Train Loss : 1.6255207806825638\n",
      "Valid Loss : 0.1435519652441144\n",
      "Epoch : 271 / 300\n",
      "Train Loss : 1.6230537127703428\n",
      "Valid Loss : 0.14522716496139765\n",
      "Epoch : 272 / 300\n",
      "Train Loss : 1.6238823477178812\n",
      "Valid Loss : 0.14378196001052856\n",
      "Epoch : 273 / 300\n",
      "Train Loss : 1.624874741770327\n",
      "Valid Loss : 0.14372436329722404\n",
      "Epoch : 274 / 300\n",
      "Train Loss : 1.6240741778165102\n",
      "Valid Loss : 0.1438357988372445\n",
      "Epoch : 275 / 300\n",
      "Train Loss : 1.6254026452079415\n",
      "Valid Loss : 0.1434743870049715\n",
      "Epoch : 276 / 300\n",
      "Train Loss : 1.6224974356591702\n",
      "Valid Loss : 0.14576872438192368\n",
      "Epoch : 277 / 300\n",
      "Train Loss : 1.6243565352633595\n",
      "Valid Loss : 0.14345357660204172\n",
      "Epoch : 278 / 300\n",
      "Train Loss : 1.6242077574133873\n",
      "Valid Loss : 0.1459794696420431\n",
      "Epoch : 279 / 300\n",
      "Train Loss : 1.6265325732529163\n",
      "Valid Loss : 0.14343872852623463\n",
      "Epoch : 280 / 300\n",
      "Train Loss : 1.6243747593834996\n",
      "Valid Loss : 0.14407044742256403\n",
      "Epoch : 281 / 300\n",
      "Train Loss : 1.6285421727225184\n",
      "Valid Loss : 0.14378648810088634\n",
      "Epoch : 282 / 300\n",
      "Train Loss : 1.6235620053485036\n",
      "Valid Loss : 0.14380164816975594\n",
      "Epoch : 283 / 300\n",
      "Train Loss : 1.6234507514163852\n",
      "Valid Loss : 0.14579605776816607\n",
      "Epoch : 284 / 300\n",
      "Train Loss : 1.624677199870348\n",
      "Valid Loss : 0.1439238442108035\n",
      "Epoch : 285 / 300\n",
      "Train Loss : 1.6207850156351924\n",
      "Valid Loss : 0.14447057247161865\n",
      "Epoch : 286 / 300\n",
      "Train Loss : 1.6240783231332898\n",
      "Valid Loss : 0.14343523792922497\n",
      "Epoch : 287 / 300\n",
      "Train Loss : 1.6233618892729282\n",
      "Valid Loss : 0.14365970809012651\n",
      "Epoch : 288 / 300\n",
      "Train Loss : 1.6255225175991654\n",
      "Valid Loss : 0.14347322192043066\n",
      "Epoch : 289 / 300\n",
      "Train Loss : 1.6234225872904062\n",
      "Valid Loss : 0.143441429361701\n",
      "Epoch : 290 / 300\n",
      "Train Loss : 1.6225162968039513\n",
      "Valid Loss : 0.1439907206222415\n",
      "Epoch : 291 / 300\n",
      "Train Loss : 1.6286575635895133\n",
      "Valid Loss : 0.14343918673694134\n",
      "Epoch : 292 / 300\n",
      "Train Loss : 1.6256759678944945\n",
      "Valid Loss : 0.14341415278613567\n",
      "Epoch : 293 / 300\n",
      "Train Loss : 1.6250088270753622\n",
      "Valid Loss : 0.1442310381680727\n",
      "Epoch : 294 / 300\n",
      "Train Loss : 1.6246872302144766\n",
      "Valid Loss : 0.1445948900654912\n",
      "Epoch : 295 / 300\n",
      "Train Loss : 1.6225118469446898\n",
      "Valid Loss : 0.14456001482903957\n",
      "Epoch : 296 / 300\n",
      "Train Loss : 1.6245469395071268\n",
      "Valid Loss : 0.14349372684955597\n",
      "Epoch : 297 / 300\n",
      "Train Loss : 1.6239639455452561\n",
      "Valid Loss : 0.14385313168168068\n",
      "Epoch : 298 / 300\n",
      "Train Loss : 1.6238911785185337\n",
      "Valid Loss : 0.14342644345015287\n",
      "Epoch : 299 / 300\n",
      "Train Loss : 1.6246393453329802\n",
      "Valid Loss : 0.14453356806188822\n",
      "Epoch : 300 / 300\n",
      "Train Loss : 1.623314443975687\n",
      "Valid Loss : 0.14352722093462944\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "best_valid_loss = float('inf')\n",
    "loss_history = []\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train() \n",
    "  train_loss = 0\n",
    "  for X, y in train_loader:\n",
    "    X_batch, y_batch = X.to(device), y.to(device)\n",
    "\n",
    "    out = model(X_batch)\n",
    "    loss = loss_fn(out, y_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  loss_history.append(train_loss / len(train_loader.dataset))\n",
    "  valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
    "  print(f'Epoch : {epoch + 1} / {num_epochs}')\n",
    "  print(f'Train Loss : {train_loss}')\n",
    "  print(f'Valid Loss : {valid_loss}')\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = valid_loss\n",
    "    torch.save(model.state_dict(), './model/1hour_best_lstm_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13592\\3127988678.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./model/1hour_best_lstm_model.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(33, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/1hour_best_lstm_model.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "  for X_batch, _ in test_loader:\n",
    "    X_batch = X_batch.to(device)\n",
    "    y_pred = model(X_batch).cpu().numpy()\n",
    "    y_pred_list.extend(y_pred)\n",
    "\n",
    "y_pred = np.array(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10580\n",
      "907\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test 크기: 4439\n",
      "y_pred 크기: 4439\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test 크기: {len(y_test)}\")\n",
    "print(f\"y_pred 크기: {len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m   y_true, y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_true), np\u001b[38;5;241m.\u001b[39marray(y_pred)\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m/\u001b[39m y_true)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 7\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_pred)\n\u001b[0;32m      9\u001b[0m mape \u001b[38;5;241m=\u001b[39m mean_absolute_percentage_error(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:112\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m--> 112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\DevTool\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\sklearn\\utils\\validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "  y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "  return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "print(f'Test MAPE: {mape:.4f}%')\n",
    "print(f'Test R-squared: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "start_index = train_size + SEQ_LENGTH\n",
    "end_index = start_index + len(y_test)\n",
    "\n",
    "datetime_range = data['datetime'][start_index:end_index].values\n",
    "if len(datetime_range) < len(y_pred):\n",
    "  print(\"Warning: Datetime range is shorter than prediction length. Adjusting range.\")\n",
    "  y_pred = y_pred[:len(datetime_range)]\n",
    "\n",
    "y_test_plot = y_test[:len(datetime_range)]\n",
    "\n",
    "plt.plot(datetime_range, y_test_plot, label='Actual')\n",
    "plt.plot(datetime_range, y_pred, label='Predicted')\n",
    "\n",
    "plt.xticks([])\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Outflow')\n",
    "plt.title('Actual vs Predicted Outflow (LSTM)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_weekly(y_true, y_pred, datetime_data, start_index, plot_title):\n",
    "  plt.figure(figsize=(20, 6))\n",
    "\n",
    "  datetime_range = datetime_data[start_index:start_index + 168].values  # 일주일 (168시간)\n",
    "\n",
    "  y_true_plot = y_true[:168]\n",
    "  y_pred_plot = y_pred[:168]\n",
    "\n",
    "  plt.plot(datetime_range, y_true_plot, label='Actual')\n",
    "  plt.plot(datetime_range, y_pred_plot, label='Predicted')\n",
    "\n",
    "  plt.xlabel('Datetime')\n",
    "  plt.ylabel('Outflow')\n",
    "  plt.title(plot_title)\n",
    "  plt.legend()\n",
    "  plt.xticks([])\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "start_index_start = train_size + SEQ_LENGTH\n",
    "plot_predictions_weekly(y_test, y_pred, data['datetime'], start_index_start, 'First Week Predictions (LSTM)')\n",
    "\n",
    "start_index_end = train_size + SEQ_LENGTH + len(y_test) - 168\n",
    "plot_predictions_weekly(y_test[-168:], y_pred[-168:], data['datetime'], len(data) - 168, 'Last Week Predictions (LSTM)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
